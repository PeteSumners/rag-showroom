# Query Decomposition

> **Intermediate Level** | Break complex queries into focused sub-questions

## Problem Statement

Users often ask complex, multi-part questions like "Compare approach X vs Y in terms of performance and cost." A single vector search returns documents vaguely related to everything, but nothing that directly addresses each specific aspect. The LLM then struggles to synthesize a coherent answer from loosely related context.

## How It Works

Query decomposition breaks complex questions into simpler sub-questions, retrieves documents for each independently, then combines the results. Each sub-query gets focused, relevant context rather than diluted results from the original broad query.

**Key Insight:** Multiple precise searches beat one vague search.

### The Process

1. **Analyze Query** - Identify if query has multiple parts/aspects
2. **Decompose** - Break into 2-5 focused sub-questions
3. **Retrieve Each** - Run separate vector searches for each sub-question
4. **Combine Results** - Merge retrieved documents (deduplicate)
5. **Generate Answer** - LLM synthesizes from comprehensive context

## Architecture

```mermaid
graph TD
    A[Complex Query:<br/>'Compare X vs Y'] --> B[LLM: Decompose]
    B --> C1[Sub-Q1: 'What is X?']
    B --> C2[Sub-Q2: 'What is Y?']
    B --> C3[Sub-Q3: 'X vs Y comparison']

    C1 --> D1[Retrieve for Q1]
    C2 --> D2[Retrieve for Q2]
    C3 --> D3[Retrieve for Q3]

    D1 --> E[Combine & Dedupe]
    D2 --> E
    D3 --> E

    E --> F[LLM: Synthesize]
    F --> G[Comprehensive Answer]

    style A fill:#e1f5ff
    style E fill:#fff3cd
    style G fill:#d4edda
```

## When to Use

‚úÖ **Good for:**
- Comparison questions ("X vs Y")
- Multi-aspect queries ("benefits AND drawbacks")
- Sequential questions ("What is X? How does it work? When to use it?")
- Analytical queries requiring diverse information sources

‚ùå **Not ideal for:**
- Simple factual questions with single answers
- When query is already focused and specific
- Time-critical systems (decomposition adds latency)
- When sub-questions would be too granular

## Trade-offs

| Aspect | Query Decomposition | Single Query |
|--------|-------------------|--------------|
| **Context Quality** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Comprehensive | ‚≠ê‚≠ê‚≠ê May miss aspects |
| **Retrieval Precision** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Focused sub-queries | ‚≠ê‚≠ê‚≠ê Broad/vague results |
| **Latency** | ‚≠ê‚≠ê‚≠ê +500ms (decompose + 3x retrieval) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Single search |
| **Cost** | ‚≠ê‚≠ê‚≠ê More API calls | ‚≠ê‚≠ê‚≠ê‚≠ê Minimal |
| **Answer Completeness** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Addresses all aspects | ‚≠ê‚≠ê‚≠ê May be incomplete |

## Code Example

Here's a simplified implementation:

```python
from typing import List

class QueryDecomposer:
    def __init__(self, llm, retriever):
        self.llm = llm
        self.retriever = retriever

    def retrieve(self, complex_query: str, top_k_per_query: int = 3) -> List[Document]:
        """
        Retrieve with query decomposition.

        Engineering decision: Break complex queries into focused sub-questions
        that each retrieve highly relevant documents, rather than one vague
        search that returns loosely related results.
        """
        # Step 1: Decompose query into sub-questions
        sub_questions = self._decompose_query(complex_query)

        # Step 2: Retrieve for each sub-question
        all_results = []
        for sub_q in sub_questions:
            results = self.retriever.search(sub_q, top_k=top_k_per_query)
            all_results.extend(results)

        # Step 3: Deduplicate (same doc might be relevant to multiple sub-questions)
        unique_results = self._deduplicate(all_results)

        return unique_results

    def _decompose_query(self, query: str) -> List[str]:
        """
        Break query into sub-questions.

        Engineering decision: Use LLM to identify distinct aspects that need
        separate retrieval. Typically 2-4 sub-questions is optimal.
        """
        prompt = f"""Break this complex question into 2-4 simpler sub-questions.
Each sub-question should focus on one specific aspect.

Question: {query}

Sub-questions:
1."""

        response = self.llm.generate(prompt)
        # Parse numbered list
        sub_questions = self._parse_list(response)
        return sub_questions
```

**Production Enhancement:** Retrieve in parallel for speed:

```python
import asyncio

async def retrieve_parallel(self, complex_query: str):
    """Retrieve all sub-questions in parallel"""
    sub_questions = self._decompose_query(complex_query)

    # Run retrievals concurrently
    tasks = [
        self.retriever.search_async(sub_q, top_k=3)
        for sub_q in sub_questions
    ]

    results = await asyncio.gather(*tasks)
    all_docs = [doc for result_set in results for doc in result_set]

    return self._deduplicate(all_docs)
```

## Running the Example

```bash
# Install dependencies
pip install rich

# Run the example
cd patterns/05-query-decomposition
python example.py

# Run tests
pytest test_example.py -v
```

**Expected Output:**
- Original complex query
- Decomposed sub-questions (2-4)
- Retrieval results for each sub-question
- Combined, deduplicated results

## Real-World Impact

**Case Study:** A customer support knowledge base added query decomposition:

- **Answer Completeness:** +45% (users rated answers as more thorough)
- **Follow-up Questions:** -38% (users got what they needed first time)
- **Latency:** +620ms (3 parallel retrievals + decomposition)
- **Context Window Usage:** +40% (more diverse docs)

**Trade-off Analysis:** Users valued comprehensive answers over speed for support queries.

## Decomposition Strategies

**Aspect-Based:**
```
"What are the benefits and drawbacks of X?"
‚Üí "What are the benefits of X?"
‚Üí "What are the drawbacks of X?"
```

**Comparison:**
```
"Compare X vs Y"
‚Üí "What is X and its features?"
‚Üí "What is Y and its features?"
‚Üí "Key differences between X and Y"
```

**Sequential:**
```
"How do I set up and use feature X?"
‚Üí "How do I install/set up feature X?"
‚Üí "How do I use feature X?"
‚Üí "What are common issues with feature X?"
```

**Causal:**
```
"Why did event X happen and what are the implications?"
‚Üí "What led to event X?"
‚Üí "What are the direct consequences of event X?"
‚Üí "What are the long-term implications?"
```

## Further Reading

- üìÑ [Least-to-Most Prompting](https://arxiv.org/abs/2205.10625) - Sequential decomposition
- üìÑ [Self-Ask: Eliciting Reasoning via Prompting](https://arxiv.org/abs/2210.03350) - Iterative decomposition
- üéì [Multi-Query Retrieval](https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever) - LangChain implementation
- üìä [Decomposed Prompting](https://arxiv.org/abs/2210.02406) - Research on breaking down tasks

---

**Previous Pattern:** [‚Üê 04-metadata-filtering](../04-metadata-filtering/README.md)
**Next Pattern:** [06-parent-child ‚Üí](../06-parent-child/README.md) | Hierarchical context retrieval
